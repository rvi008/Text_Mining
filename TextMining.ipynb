{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP de Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading dataset\")\n",
    "filenames_neg = sorted(glob(op.join('data', 'imdb1', 'neg', '*.txt'))) # Assumes that data are located in data/imdb1/neg\n",
    "filenames_pos = sorted(glob(op.join('data', 'imdb1', 'pos', '*.txt'))) # Assumes that data are located in data/imdb1/neg\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Compléter la fonction count_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    voc = {}\n",
    "    wordidx = 0\n",
    "    counts = np.zeros((len(texts), 0))\n",
    "    for textidx,text  in enumerate(texts):\n",
    "        #print(textidx,\" / \",len(texts), \" documents\\r\" )\n",
    "        text = re.sub('\\W+', \" \", text).lower().strip().split(\" \") # Removes non-words characters and tokenize text\n",
    "        for word in text:\n",
    "                # If the word's already in the vocabulary dict, increment the count of the word in the counts matrix\n",
    "                if word in voc: # for the current document\n",
    "                    counts[textidx, voc[word]] += 1\n",
    "                # Else add the new word to the vocabulary dict and set it's index in the counts matrix\n",
    "                else:\n",
    "                    voc[word] = wordidx \n",
    "                    counts = np.c_[counts, np.zeros(len(texts))] # Add a new column in counts for the new word\n",
    "                    # increment the count in counts for the current word in the current document\n",
    "                    counts[textidx, wordidx] += 1 \n",
    "                    wordidx += 1 # Increment the index for the next new word\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  /  2000  documents\n",
      "1  /  2000  documents\n",
      "2  /  2000  documents\n",
      "3  /  2000  documents\n",
      "4  /  2000  documents\n",
      "5  /  2000  documents\n",
      "6  /  2000  documents\n",
      "7  /  2000  documents\n",
      "8  /  2000  documents\n",
      "9  /  2000  documents\n",
      "10  /  2000  documents\n",
      "11  /  2000  documents\n",
      "12  /  2000  documents\n",
      "13  /  2000  documents\n",
      "14  /  2000  documents\n",
      "15  /  2000  documents\n",
      "16  /  2000  documents\n",
      "17  /  2000  documents\n",
      "18  /  2000  documents\n",
      "19  /  2000  documents\n",
      "20  /  2000  documents\n",
      "21  /  2000  documents\n",
      "22  /  2000  documents\n",
      "23  /  2000  documents\n",
      "24  /  2000  documents\n",
      "25  /  2000  documents\n",
      "26  /  2000  documents\n",
      "27  /  2000  documents\n",
      "28  /  2000  documents\n",
      "29  /  2000  documents\n",
      "30  /  2000  documents\n",
      "31  /  2000  documents\n",
      "32  /  2000  documents\n",
      "33  /  2000  documents\n",
      "34  /  2000  documents\n",
      "35  /  2000  documents\n",
      "36  /  2000  documents\n",
      "37  /  2000  documents\n",
      "38  /  2000  documents\n",
      "39  /  2000  documents\n",
      "40  /  2000  documents\n",
      "41  /  2000  documents\n",
      "42  /  2000  documents\n",
      "43  /  2000  documents\n",
      "44  /  2000  documents\n",
      "45  /  2000  documents\n",
      "46  /  2000  documents\n",
      "47  /  2000  documents\n",
      "48  /  2000  documents\n",
      "49  /  2000  documents\n",
      "50  /  2000  documents\n",
      "51  /  2000  documents\n",
      "52  /  2000  documents\n",
      "53  /  2000  documents\n",
      "54  /  2000  documents\n",
      "55  /  2000  documents\n",
      "56  /  2000  documents\n",
      "57  /  2000  documents\n",
      "58  /  2000  documents\n",
      "59  /  2000  documents\n",
      "60  /  2000  documents\n",
      "61  /  2000  documents\n",
      "62  /  2000  documents\n",
      "63  /  2000  documents\n",
      "64  /  2000  documents\n",
      "65  /  2000  documents\n",
      "66  /  2000  documents\n",
      "67  /  2000  documents\n",
      "68  /  2000  documents\n",
      "69  /  2000  documents\n",
      "70  /  2000  documents\n",
      "71  /  2000  documents\n",
      "72  /  2000  documents\n",
      "73  /  2000  documents\n",
      "74  /  2000  documents\n",
      "75  /  2000  documents\n",
      "76  /  2000  documents\n",
      "77  /  2000  documents\n",
      "78  /  2000  documents\n",
      "79  /  2000  documents\n",
      "80  /  2000  documents\n",
      "81  /  2000  documents\n",
      "82  /  2000  documents\n",
      "83  /  2000  documents\n",
      "84  /  2000  documents\n",
      "85  /  2000  documents\n",
      "86  /  2000  documents\n",
      "87  /  2000  documents\n",
      "88  /  2000  documents\n",
      "89  /  2000  documents\n",
      "90  /  2000  documents\n",
      "91  /  2000  documents\n",
      "92  /  2000  documents\n",
      "93  /  2000  documents\n",
      "94  /  2000  documents\n",
      "95  /  2000  documents\n",
      "96  /  2000  documents\n",
      "97  /  2000  documents\n",
      "98  /  2000  documents\n",
      "99  /  2000  documents\n",
      "100  /  2000  documents\n",
      "101  /  2000  documents\n",
      "102  /  2000  documents\n",
      "103  /  2000  documents\n",
      "104  /  2000  documents\n",
      "105  /  2000  documents\n",
      "106  /  2000  documents\n",
      "107  /  2000  documents\n",
      "108  /  2000  documents\n",
      "109  /  2000  documents\n",
      "110  /  2000  documents\n",
      "111  /  2000  documents\n",
      "112  /  2000  documents\n",
      "113  /  2000  documents\n",
      "114  /  2000  documents\n",
      "115  /  2000  documents\n",
      "116  /  2000  documents\n",
      "117  /  2000  documents\n",
      "118  /  2000  documents\n",
      "119  /  2000  documents\n",
      "120  /  2000  documents\n",
      "121  /  2000  documents\n",
      "122  /  2000  documents\n",
      "123  /  2000  documents\n",
      "124  /  2000  documents\n",
      "125  /  2000  documents\n",
      "126  /  2000  documents\n",
      "127  /  2000  documents\n",
      "128  /  2000  documents\n",
      "129  /  2000  documents\n",
      "130  /  2000  documents\n",
      "131  /  2000  documents\n",
      "132  /  2000  documents\n",
      "133  /  2000  documents\n",
      "134  /  2000  documents\n",
      "135  /  2000  documents\n",
      "136  /  2000  documents\n",
      "137  /  2000  documents\n",
      "138  /  2000  documents\n",
      "139  /  2000  documents\n",
      "140  /  2000  documents\n",
      "141  /  2000  documents\n",
      "142  /  2000  documents\n",
      "143  /  2000  documents\n",
      "144  /  2000  documents\n",
      "145  /  2000  documents\n",
      "146  /  2000  documents\n",
      "147  /  2000  documents\n",
      "148  /  2000  documents\n",
      "149  /  2000  documents\n",
      "150  /  2000  documents\n",
      "151  /  2000  documents\n",
      "152  /  2000  documents\n",
      "153  /  2000  documents\n",
      "154  /  2000  documents\n",
      "155  /  2000  documents\n",
      "156  /  2000  documents\n",
      "157  /  2000  documents\n",
      "158  /  2000  documents\n",
      "159  /  2000  documents\n",
      "160  /  2000  documents\n",
      "161  /  2000  documents\n",
      "162  /  2000  documents\n",
      "163  /  2000  documents\n",
      "164  /  2000  documents\n",
      "165  /  2000  documents\n",
      "166  /  2000  documents\n",
      "167  /  2000  documents\n",
      "168  /  2000  documents\n",
      "169  /  2000  documents\n",
      "170  /  2000  documents\n",
      "171  /  2000  documents\n",
      "172  /  2000  documents\n",
      "173  /  2000  documents\n",
      "174  /  2000  documents\n",
      "175  /  2000  documents\n",
      "176  /  2000  documents\n",
      "177  /  2000  documents\n",
      "178  /  2000  documents\n",
      "179  /  2000  documents\n",
      "180  /  2000  documents\n",
      "181  /  2000  documents\n",
      "182  /  2000  documents\n",
      "183  /  2000  documents\n",
      "184  /  2000  documents\n",
      "185  /  2000  documents\n",
      "186  /  2000  documents\n",
      "187  /  2000  documents\n",
      "188  /  2000  documents\n",
      "189  /  2000  documents\n",
      "190  /  2000  documents\n",
      "191  /  2000  documents\n"
     ]
    }
   ],
   "source": [
    "vocabulary, X = count_words(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
