{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP de Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from glob import glob\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading dataset\")\n",
    "filenames_neg = sorted(glob(op.join('data', 'imdb1', 'neg', '*.txt'))) # Assumes that data are located in data/imdb1/neg\n",
    "filenames_pos = sorted(glob(op.join('data', 'imdb1', 'pos', '*.txt'))) # Assumes that data are located in data/imdb1/neg\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Compléter la fonction count_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    voc = {}\n",
    "    uniq = set()\n",
    "    # Determine total number of uniques words in the whole corpus of texts\n",
    "    for text in texts :\n",
    "        words = set(re.findall(r\"\\w+\",text.lower()))\n",
    "        uniq = uniq.union(words)\n",
    "    counts = np.zeros((len(texts), len(uniq)))\n",
    "    wordidx = 0\n",
    "    for textidx,text  in enumerate(texts):\n",
    "        #print(textidx,\" / \",len(texts), \" documents\\r\" )\n",
    "        text = re.findall(r\"\\w+\",text.lower()) # Removes non-words characters and tokenize text\n",
    "        for word in text:\n",
    "                # If the word's already in the vocabulary dict, increment the count of the word in the counts matrix\n",
    "                if word in voc: # for the current document\n",
    "                    counts[textidx, voc[word]] += 1\n",
    "                # Else add the new word to the vocabulary dict and set it's index in the counts matrix\n",
    "                else:\n",
    "                    voc[word] = wordidx \n",
    "                    # increment the count in counts for the current word in the current document\n",
    "                    counts[textidx, wordidx] += 1 \n",
    "                    wordidx += 1 # Increment the index for the next new word\n",
    "    return voc, counts    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary, X = count_words(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 39696)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction marche correctement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Expliquer la classification obtenue dans le fichier poldata.readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) implémenter le classifieur Bayésien naïf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self): # No modification, everything is inherited from BaseEstimator et ClassifierMixin\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        vocabulary, counts = count_words(X)\n",
    "        nDocs = len(counts)\n",
    "        C = set(y)\n",
    "        prior = {}\n",
    "        tct = np.zeros([len(C), len(vocabulary)])\n",
    "        array_condprob = np.copy(tct)\n",
    "        cond_prob = {}\n",
    "        for i,c in enumerate(C):\n",
    "            # Get lines' index according to the membership of the current class\n",
    "            ixc = np.where(y == c)[0]\n",
    "            nDocsInc = len(ixc)\n",
    "            prior[c] = nDocsInc / nDocs # class frequencies by document\n",
    "            # Gets all words associated to the current class from counts matrix\n",
    "            countsByClass = counts[ixc, :] \n",
    "            # Sum to get the number of word for each class\n",
    "            tct[i, :] = np.sum(countsByClass, axis=0)\n",
    "            # Compute the \n",
    "            array_cond_prob[i, :] = (tct[i, :] + 1) / np.sum(tct[i, :] + 1)\n",
    "            cond_prob[c] = array_cond_prob[i, :]\n",
    "        self.vocabulary = vocabulary\n",
    "        self.prior = prior\n",
    "        self.cond_prob = cond_prob\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
