{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP de Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from glob import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading dataset\")\n",
    "filenames_neg = sorted(glob(op.join('data', 'imdb1', 'neg', '*.txt'))) # Assumes that data are located in data/imdb1/neg\n",
    "filenames_pos = sorted(glob(op.join('data', 'imdb1', 'pos', '*.txt'))) # Assumes that data are located in data/imdb1/neg\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Compléter la fonction count_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    voc = {}\n",
    "    uniq = set()\n",
    "    # Determine total number of uniques words in the whole corpus of texts\n",
    "    for text in texts :\n",
    "        words = set(re.findall(r\"\\w+\",text.lower()))\n",
    "        uniq = uniq.union(words)\n",
    "    counts = np.zeros((len(texts), len(uniq)))\n",
    "    wordidx = 0\n",
    "    for textidx,text  in enumerate(texts):\n",
    "        #print(textidx,\" / \",len(texts), \" documents\\r\" )\n",
    "        text = re.findall(r\"\\w+\",text.lower()) # Removes non-words characters and tokenize text\n",
    "        for word in text:\n",
    "                # If the word's already in the vocabulary dict, increment the count of the word in the counts matrix\n",
    "                if word in voc: # for the current document\n",
    "                    counts[textidx, voc[word]] += 1\n",
    "                # Else add the new word to the vocabulary dict and set it's index in the counts matrix\n",
    "                else:\n",
    "                    voc[word] = wordidx \n",
    "                    # increment the count in counts for the current word in the current document\n",
    "                    counts[textidx, wordidx] += 1 \n",
    "                    wordidx += 1 # Increment the index for the next new word\n",
    "    return voc, counts    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary, X = count_words(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 39696)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction marche correctement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Expliquer la classification obtenue dans le fichier poldata.readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) implémenter le classifieur Bayésien naïf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self): # No modification, everything is inherited from BaseEstimator et ClassifierMixin\n",
    "        self.prior = {}\n",
    "        self.vocabulary = {}\n",
    "        self.cond_prob = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        vocabulary, counts = count_words(X)\n",
    "        nDocs = len(counts)\n",
    "        C = set(y)\n",
    "        prior = {}\n",
    "        tct = np.zeros([len(C), len(vocabulary)])\n",
    "        array_condprob = np.copy(tct)\n",
    "        cond_prob = {}\n",
    "        for i,c in enumerate(C):\n",
    "            # Get lines' index according to the membership of the current class\n",
    "            ixc = np.where(y == c)[0]\n",
    "            nDocsInc = len(ixc)\n",
    "            prior[c] = nDocsInc / nDocs # class frequencies by document\n",
    "            # Gets all words associated to the current class from counts matrix\n",
    "            countsByClass = counts[ixc, :] \n",
    "            # Sum to get the number of word for each class\n",
    "            tct[i, :] = np.sum(countsByClass, axis=0)\n",
    "            # Computes the conditional probability for the word to belong to the current class \n",
    "            array_condprob[i, :] = (tct[i, :] + 1) / np.sum(tct[i, :] + 1)\n",
    "            # Stores this probabilities into a dict with key : class and value : the probability for each word\n",
    "            cond_prob[c] = array_condprob[i, :]\n",
    "        self.vocabulary = vocabulary\n",
    "        self.prior = prior\n",
    "        self.cond_prob = cond_prob\n",
    "    \n",
    "    def extract_tokens(vocabulary, doc):\n",
    "        tokens = re.findall(r\"\\w+\", doc.lower())\n",
    "        # New words missing from the training set are left over\n",
    "        return [token for token in tokens if token in vocabulary.keys()] \n",
    "        \n",
    "    def predict(self, X):\n",
    "        results = np.zeros(len(X))\n",
    "        for i,text in enumerate(X):\n",
    "            # Gets all the word from the current testing doc\n",
    "            w = NB.extract_tokens(self.vocabulary, text)\n",
    "            score, maxprob, maxclass, = 0, - math.inf, 0\n",
    "            for c in self.prior:\n",
    "                # Initializes the score based on the a priori probability of belonging to a class\n",
    "                score = np.log(self.prior[c])\n",
    "                for word in w:\n",
    "                    # Gets the index of the word in the train's counts matrix\n",
    "                    index = self.vocabulary[word]\n",
    "                    # Computes the score of the word by adding to it the log of its conditionnal probability of belonging\n",
    "                    # to the class from the train's counts matrix\n",
    "                    score += np.log(self.cond_prob[c][index])    \n",
    "                if score > maxprob:\n",
    "                    maxprob, maxclass  = score, c\n",
    "            # stores for the current document, the predicted class according to the maximum conditionnal probability        \n",
    "            results[i] = maxclass\n",
    "        return results\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le PDF pour l'interpretation de l'algorithme.\n",
    "Test du classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n"
     ]
    }
   ],
   "source": [
    "nb = NB()\n",
    "nb.fit(texts[::2], y[::2])\n",
    "print(nb.score(texts[1::2], y[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classifieur fonctionne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Tester le classifieur avec une validation croisée à 5 passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons la cross validation de SKLEARN pour tester le classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(nb,texts[1::2], y[1::2], cv = 10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont plutôts corrects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Filtrer les stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_words(texts, stopwords = None):\n",
    "    voc = {}\n",
    "    uniq = set()\n",
    "    # Determine total number of uniques words in the whole corpus of texts\n",
    "    for text in texts :\n",
    "        words = re.findall(r\"\\w+\",text.lower())\n",
    "        words = set([word for word in words if word not in stopwords])\n",
    "        uniq = uniq.union(words)\n",
    "    counts = np.zeros((len(texts), len(uniq)))\n",
    "    wordidx = 0\n",
    "    for textidx,text  in enumerate(texts):\n",
    "        text = re.findall(r\"\\w+\",text.lower()) # Removes non-words characters and tokenize text\n",
    "        text = [word for word in text if word not in stopwords]\n",
    "        for word in text:\n",
    "                # If the word's already in the vocabulary dict, increment the count of the word in the counts matrix\n",
    "                if word in voc: # for the current document\n",
    "                    counts[textidx, voc[word]] += 1\n",
    "                # Else add the new word to the vocabulary dict and set it's index in the counts matrix\n",
    "                else:\n",
    "                    voc[word] = wordidx \n",
    "                    # increment the count in counts for the current word in the current document\n",
    "                    counts[textidx, wordidx] += 1 \n",
    "                    wordidx += 1 # Increment the index for the next new word\n",
    "    return voc, counts    \n",
    "\n",
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self): # No modification, everything is inherited from BaseEstimator et ClassifierMixin\n",
    "        self.prior = {}\n",
    "        self.vocabulary = {}\n",
    "        self.cond_prob = {}\n",
    "\n",
    "    def fit(self, X, y, stop=None):\n",
    "        vocabulary, counts = count_words(X,stop)\n",
    "        nDocs = len(counts)\n",
    "        C = set(y)\n",
    "        prior = {}\n",
    "        tct = np.zeros([len(C), len(vocabulary)])\n",
    "        array_condprob = np.copy(tct)\n",
    "        cond_prob = {}\n",
    "        for i,c in enumerate(C):\n",
    "            # Get lines' index according to the membership of the current class\n",
    "            ixc = np.where(y == c)[0]\n",
    "            nDocsInc = len(ixc)\n",
    "            prior[c] = nDocsInc / nDocs # class frequencies by document\n",
    "            # Gets all words associated to the current class from counts matrix\n",
    "            countsByClass = counts[ixc, :] \n",
    "            # Sum to get the number of word for each class\n",
    "            tct[i, :] = np.sum(countsByClass, axis=0)\n",
    "            # Computes the conditional probability for the word to belong to the current class \n",
    "            array_condprob[i, :] = (tct[i, :] + 1) / np.sum(tct[i, :] + 1)\n",
    "            # Stores this probabilities into a dict with key : class and value : the probability for each word\n",
    "            cond_prob[c] = array_condprob[i, :]\n",
    "        self.vocabulary = vocabulary\n",
    "        self.prior = prior\n",
    "        self.cond_prob = cond_prob\n",
    "    \n",
    "    def extract_tokens(vocabulary, doc):\n",
    "        tokens = re.findall(r\"\\w+\", doc.lower())\n",
    "        # New words missing from the training set are left over\n",
    "        return [token for token in tokens if token in vocabulary.keys()] \n",
    "        \n",
    "    def predict(self, X):\n",
    "        results = np.zeros(len(X))\n",
    "        for i,text in enumerate(X):\n",
    "            # Gets all the word from the current testing doc\n",
    "            w = NB.extract_tokens(self.vocabulary, text)\n",
    "            score, maxprob, maxclass, = 0, - math.inf, 0\n",
    "            for c in self.prior:\n",
    "                # Initializes the score based on the a priori probability of belonging to a class\n",
    "                score = np.log(self.prior[c])\n",
    "                for word in w:\n",
    "                    # Gets the index of the word in the train's counts matrix\n",
    "                    index = self.vocabulary[word]\n",
    "                    # Computes the score of the word by adding to it the log of its conditionnal probability of belonging\n",
    "                    # to the class from the train's counts matrix\n",
    "                    score += np.log(self.cond_prob[c][index])    \n",
    "                if score > maxprob:\n",
    "                    maxprob, maxclass  = score, c\n",
    "            # stores for the current document, the predicted class according to the maximum conditionnal probability        \n",
    "            results[i] = maxclass\n",
    "        return results\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "    \n",
    "f = open(\"data/english.stop\")\n",
    "stop = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retestons les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812\n"
     ]
    }
   ],
   "source": [
    "nb = NB()\n",
    "nb.fit(texts[::2], y[::2], stop)\n",
    "print(nb.score(texts[1::2], y[1::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec une cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7765\n",
      "0.0180693109996\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for _ in range(5):\n",
    "    randidx = np.random.randint(0, len(texts), int(len(texts)/5))\n",
    "    subset_train = [texts[i] for i in randidx]\n",
    "    labels_train = [y[i] for i in randidx]\n",
    "    nb.fit(subset_train, labels_train, stop)\n",
    "    randidx = np.random.randint(0, len(texts), int(len(texts)/5))\n",
    "    subset_test = [texts[i] for i in randidx]\n",
    "    labels_test = [y[i] for i in randidx]\n",
    "    scores.append(nb.score(subset_test, labels_test))\n",
    "print(np.mean(scores))\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score obtenu est quasi identique au précédent mais on note qu'il est beaucoup plus stable car la déviation standard est petite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Utilisation de MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61 (+/- 0.15)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=stop)\n",
    "nb = MultinomialNB()\n",
    "naiveBayes = Pipeline([('CountVectorizer', vectorizer),('MultinomialNB',nb)])\n",
    "naiveBayes.set_params(CountVectorizer__analyzer = 'char_wb').fit(texts[::2], y[::2])\n",
    "scores = cross_val_score(naiveBayes,texts[1::2], y[1::2], cv = 5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "naiveBayes.set_params(CountVectorizer__analyzer = 'word').fit(texts[::2], y[::2])\n",
    "scores = cross_val_score(naiveBayes,texts[1::2], y[1::2], cv = 5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "naiveBayes.set_params(CountVectorizer__ngram_range = (1,2) ).fit(texts[::2], y[::2])\n",
    "scores = cross_val_score(naiveBayes,texts[1::2], y[1::2], cv = 5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Test d'autres algorithmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stop)\n",
    "svm = LinearSVC()\n",
    "predictor = Pipeline([('CountVectorizer', vectorizer),('LinearSVC',svm)]).fit(texts[::2], y[::2])\n",
    "scores = cross_val_score(predictor,texts[1::2], y[1::2], cv = 5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "predictor = Pipeline([('CountVectorizer', vectorizer),('LogisticRegression',log)]).fit(texts[::2], y[::2])\n",
    "scores = cross_val_score(predictor,texts[1::2], y[1::2], cv = 5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont quasiment les mêmes pour ces deux algorithmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Utilisation de NLTK pour raciniser les mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from nltk import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "stemmer = SnowballStemmer('english')\n",
    "# Declaration of a tokenizer to be used with sklearn\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize, stop_words=stop)\n",
    "nb = MultinomialNB()\n",
    "naiveBayes = Pipeline([('CountVectorizer', vectorizer),('MultinomialNB',nb)])\n",
    "naiveBayes.set_params(CountVectorizer__analyzer = 'word').fit(texts[::2], y[::2])\n",
    "scores = cross_val_score(naiveBayes,texts[1::2], y[1::2], cv = 5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La précision ne s'améliore pas beaucoup, essayons avec un autre classifieur, le SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "predictor = Pipeline([('CountVectorizer', vectorizer),('LinearSVC',svm)]).fit(texts[::2], y[::2])\n",
    "scores = cross_val_score(predictor,texts[1::2], y[1::2], cv = 5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La performance est identique à la précédente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Utilisation de pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def pos_tag_tokens(tokens) :\n",
    "    tagged = pos_tag(tokens)\n",
    "    filtered = [token[0] for token in tokens if togen[1] in ['NN','VB','JJ','RB','VBP']]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    tagged = pos_tag_tokens(stems)\n",
    "    return tagged\n",
    "\n",
    "\n",
    "svm = LinearSVC()\n",
    "predictor = Pipeline([('CountVectorizer', vectorizer),('LinearSVC',svm)]).fit(texts[::2], y[::2])\n",
    "scores = cross_val_score(predictor,texts[1::2], y[1::2], cv = 5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
